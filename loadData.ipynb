{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "O Free Spoken Digit Dataset é uma coleção de gravações de áudio de declarações de dígitos (“zero” a “nove”) de diferentes pessoas.\n",
    "O objetivo desta competição é identificar corretamente o dígito que está sendo pronunciado em cada gravação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.audioProcessor import AudioProcessor\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioProcessor()\n",
    "X1 = []\n",
    "y = []\n",
    "for audio_file in audio.audio_files:\n",
    "    X1.append(audio_file.sample)\n",
    "    y.append(int(audio_file.label))\n",
    "print(X1[0])\n",
    "\n",
    "files = 'data/recordings/'\n",
    "ds_files = listdir(files)\n",
    "\n",
    "X = []\n",
    "for file in ds_files:\n",
    "    label = int(file.split(\"_\")[0])\n",
    "    rate, data = wavfile.read(join(files, file))\n",
    "    X.append(data.astype(np.float32))\n",
    "\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "O problema está bem equilibrado: para cada uma das classes temos 300 amostras no conjunto de dados.\n",
    "Todas as gravações são amostradas na taxa de 8 kHZ\n",
    "\n",
    "Os sinais de áudio têm comprimentos diferentes.\n",
    "Alguns deles têm intervalos iniciais e de silêncio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = audio.show_length_distribution(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casos extermos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_audio = np.argmax([len(x) for x in X])\n",
    "print(audio.get_longest_audio())\n",
    "print(X[longest_audio]/8000)\n",
    "plt.plot(X[longest_audio])\n",
    "plt.title(\"Longest audio signal\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "display(ipd.Audio(X[longest_audio], rate=audio.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_audio = np.argmin([len(x) for x in X])\n",
    "plt.plot(X[shortest_audio])\n",
    "plt.title(\"Shortest audio signal\")\n",
    "plt.show()\n",
    "\n",
    "display(ipd.Audio(X[shortest_audio], rate= audio.sample_rate))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de recursos do domínio do tempo:\n",
    "Vamos remover o silêncio inicial e final dos sinais para ver se obtemos uma distribuição diferente de comprimento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 0\n",
    "for x in X:\n",
    "    max_size = max(max_size, x.shape[0])\n",
    "    \n",
    "print('Max sizing before:', max_size)\n",
    "\n",
    "# trim silence\n",
    "X = [audio.remove_silence(x) for x in X ]\n",
    "\n",
    "\n",
    "max_size = 0\n",
    "for x in X:\n",
    "    max_size = max(max_size, x.shape[0])\n",
    "    \n",
    "print('Max sizing after:', max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = audio.show_length_distribution(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding to max size\n",
    "#! ATENÇÃO NÃO É O MESMO PADDING DO AUDIOPROCESSOR\n",
    "X = [ np.pad(x, (0, max_size - x.shape[0])) for x in X ] \n",
    "\n",
    "plt.plot(X[longest_audio])\n",
    "plt.title(\"Longest audio signal after trimming\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "display(ipd.Audio(X[longest_audio], rate=audio.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[shortest_audio])\n",
    "plt.title(\"Shortest audio signal after trimming\")\n",
    "plt.show()\n",
    "\n",
    "display(ipd.Audio(X[shortest_audio], rate=audio.sample_rate))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectorgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "def freq_mask(spec):\n",
    "    return tfio.audio.freq_mask(spec, param=2).numpy()\n",
    "\n",
    "def time_mask(spec):\n",
    "    return tfio.audio.time_mask(spec, param=2).numpy()\n",
    "\n",
    "\n",
    "def mel_spectrogram(waveform):\n",
    "    spec = librosa.feature.melspectrogram(y=waveform, sr=8000)\n",
    "    return librosa.power_to_db(spec, ref=np.max)\n",
    "\n",
    "\n",
    "def mfcc_spectrogram(waveform):\n",
    "    return librosa.feature.mfcc(y=waveform, sr=8000)\n",
    "\n",
    "\n",
    "def plot_spectrogram(spectrogram, ax):\n",
    "  # Convert to frequencies to log scale and transpose so that the time is\n",
    "  # represented in the x-axis (columns).\n",
    "  log_spec = np.log(spectrogram.T)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)\n",
    "\n",
    "\n",
    "\n",
    "def stft_spectrogram(waveform):\n",
    "  # Padding for files with less than 16000 samples\n",
    "  zero_padding = tf.zeros([max_size] - tf.shape(waveform), dtype=tf.float32)\n",
    "\n",
    "  # Concatenate audio with padding so that all audio clips will be of the \n",
    "  # same length\n",
    "  waveform = tf.cast(waveform, tf.float32)\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  return spectrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = X[longest_audio]\n",
    "\n",
    "S = mel_spectrogram(wave)\n",
    "print(S.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "img = librosa.display.specshow(S, x_axis='time',\n",
    "                         y_axis='mel', sr=8000, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency Spectrogram')\n",
    "\n",
    "\n",
    "\n",
    "S = mfcc_spectrogram(wave)\n",
    "print(S.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "img = librosa.display.specshow(S, x_axis='time',\n",
    "                         y_axis='mel', sr=8000, ax=ax[0])\n",
    "fig.colorbar(img, ax=ax[0], format='%+2.0f dB')\n",
    "ax[0].set(title='MFCC Spectrogram')\n",
    "\n",
    "\n",
    "S2 = freq_mask(time_mask(S))\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "img = librosa.display.specshow(S2, x_axis='time',\n",
    "                         y_axis='mel', sr=8000, ax=ax[1])\n",
    "\n",
    "fig.colorbar(img, ax=ax[1], format='%+2.0f dB')\n",
    "fig.set_size_inches(15, 5)\n",
    "ax[1].set(title='MFCC Spectrogram with Freq. and Time Mask')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = X[longest_audio]\n",
    "\n",
    "spectrogram = stft_spectrogram(waveform).numpy()\n",
    "\n",
    "#spectrogram.resize(32, 32)\n",
    "print(spectrogram.shape)\n",
    "\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "axes[0].plot(timescale, waveform)\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, max_size])\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('STFT Spectrogram')\n",
    "\n",
    "plot_spectrogram(spectrogram, axes[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6807d9fb81799ae5657feca825e590051392ad123fb9d818161faf1e20a1718b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
